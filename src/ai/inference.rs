use super::*;
use anyhow::Result;
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock};
use tokio::time::{sleep, Duration};

pub struct InferenceEngine {
    providers: Vec<Box<dyn AIProvider + Send + Sync>>,
    active_provider: Option<usize>,
    config: Arc<RwLock<InferenceConfig>>,
}

pub struct BasicDemoProvider;

impl AIProvider for BasicDemoProvider {
    fn name(&self) -> &str { "Intelligent Demo Mode" }
    fn is_available(&self) -> bool { true }
    fn generate_response(&mut self, messages: &[ChatMessage]) -> Result<String> {
        let last = messages.iter().rev().find(|m| matches!(m.role, MessageRole::User));
        let Some(msg) = last else {
            return Ok("Hello! I'm RIA AI Assistant running in demo mode. üöÄ\n\nI can help you with questions, coding, explanations, and more! While I'm in demo mode, I'll provide helpful contextual responses. For enhanced AI capabilities, load an ONNX model from the üß† Models tab.\n\nHow can I help you today?".to_string());
        };
        
        let content = msg.content.to_lowercase();
        let response = self.generate_intelligent_response(&content, &msg.content, messages);
        Ok(response)
    }
    
    fn get_model_info(&self) -> Result<std::collections::HashMap<String,String>> {
        let mut m = std::collections::HashMap::new();
        m.insert("provider".into(), "Intelligent Demo Mode".into());
        m.insert("capabilities".into(), "Contextual responses, coding help, explanations".into());
        m.insert("status".into(), "Active (fallback mode)".into());
        Ok(m)
    }

    fn as_any(&self) -> &dyn std::any::Any { self }
}

impl BasicDemoProvider {
    fn generate_intelligent_response(&self, content_lower: &str, original_content: &str, messages: &[ChatMessage]) -> String {
        // NPU and hardware acceleration questions
        if content_lower.contains("npu") || content_lower.contains("neural processing") {
            return "üß† **NPU (Neural Processing Unit) Information**\n\nHere's a simple Rust example for NPU-aware code:\n\n```rust\nuse std::arch::is_x86_feature_detected;\n\n// Check for AI acceleration capabilities\nfn detect_npu_support() -> bool {\n    // Check for Intel AMX (Advanced Matrix Extensions)\n    if is_x86_feature_detected!(\"avx512f\") {\n        println!(\"üü¢ AVX-512 detected - good for AI workloads\");\n        return true;\n    }\n    \n    // Check system info for NPU\n    if std::env::var(\"OPENVINO_NPU_AVAILABLE\").is_ok() {\n        println!(\"üü¢ OpenVINO NPU runtime detected\");\n        return true;\n    }\n    \n    false\n}\n\nfn main() {\n    if detect_npu_support() {\n        println!(\"üöÄ NPU acceleration available!\");\n    } else {\n        println!(\"‚ö° Using CPU optimization\");\n    }\n}\n```\n\n**NPU Integration Tips:**\n‚Ä¢ Use OpenVINO toolkit for Intel NPUs\n‚Ä¢ Check for DirectML on Windows\n‚Ä¢ Leverage ONNX Runtime execution providers\n‚Ä¢ Consider model quantization (INT8/FP16)\n\n**Current RIA Status**: ONNX Runtime version mismatch preventing NPU usage. The app expects v1.22+ but system has v1.17.1.".to_string();
        }
        
        // Specific Rust code requests
        if (content_lower.contains("write") || content_lower.contains("code")) && content_lower.contains("rust") && content_lower.contains("npu") {
            return "ü¶Ä **Rust NPU Integration Code Example**\n\n```rust\nuse std::sync::Arc;\nuse tokio::sync::RwLock;\n\n// NPU-aware inference structure\n#[derive(Debug)]\npub struct NpuInference {\n    device_type: DeviceType,\n    model_path: String,\n    optimization_level: OptimLevel,\n}\n\n#[derive(Debug, Clone)]\nenum DeviceType {\n    CPU,\n    NPU,\n    GPU,\n    Auto,\n}\n\n#[derive(Debug)]\nenum OptimLevel {\n    Basic,\n    Aggressive,\n    NPUOptimized,\n}\n\nimpl NpuInference {\n    pub fn new() -> Self {\n        Self {\n            device_type: Self::detect_best_device(),\n            model_path: String::new(),\n            optimization_level: OptimLevel::NPUOptimized,\n        }\n    }\n    \n    fn detect_best_device() -> DeviceType {\n        // Check for Intel NPU\n        if std::env::var(\"INTEL_NPU_AVAILABLE\").is_ok() {\n            return DeviceType::NPU;\n        }\n        \n        // Check for GPU acceleration\n        if std::env::var(\"CUDA_VISIBLE_DEVICES\").is_ok() {\n            return DeviceType::GPU;\n        }\n        \n        DeviceType::CPU\n    }\n    \n    pub async fn load_model(&mut self, path: &str) -> Result<(), Box<dyn std::error::Error>> {\n        println!(\"üîÑ Loading model on {:?}...\", self.device_type);\n        self.model_path = path.to_string();\n        \n        match self.device_type {\n            DeviceType::NPU => {\n                println!(\"‚ö° Initializing NPU cores...\");\n                // NPU-specific initialization\n                self.init_npu_cores().await?;\n            },\n            DeviceType::GPU => {\n                println!(\"üéÆ Using GPU acceleration...\");\n            },\n            DeviceType::CPU => {\n                println!(\"üñ•Ô∏è Using optimized CPU inference...\");\n            },\n            DeviceType::Auto => {\n                println!(\"ü§ñ Auto-detecting best device...\");\n            }\n        }\n        \n        Ok(())\n    }\n    \n    async fn init_npu_cores(&self) -> Result<(), Box<dyn std::error::Error>> {\n        // Initialize NPU cores with OpenVINO or similar\n        println!(\"üß† NPU cores initialized successfully\");\n        Ok(())\n    }\n    \n    pub async fn infer(&self, input: &[f32]) -> Result<Vec<f32>, Box<dyn std::error::Error>> {\n        match self.device_type {\n            DeviceType::NPU => {\n                println!(\"üöÄ Running inference on NPU cores...\");\n                // NPU inference logic here\n                Ok(vec![0.95, 0.03, 0.02]) // Mock output\n            },\n            _ => {\n                println!(\"‚ö° Running inference on {:#?}...\", self.device_type);\n                Ok(vec![0.85, 0.10, 0.05]) // Mock output\n            }\n        }\n    }\n}\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    let mut npu_engine = NpuInference::new();\n    npu_engine.load_model(\"./model.onnx\").await?;\n    \n    let input_data = vec![1.0, 2.0, 3.0, 4.0];\n    let output = npu_engine.infer(&input_data).await?;\n    \n    println!(\"üéØ Inference result: {:#?}\", output);\n    Ok(())\n}\n```\n\n**This code demonstrates:**\n‚úÖ Device detection and NPU preference\n‚úÖ Async model loading with device optimization\n‚úÖ NPU core initialization\n‚úÖ Device-specific inference paths\n‚úÖ Error handling and logging\n\n**To integrate NPUs properly:**\n‚Ä¢ Install compatible ONNX Runtime (1.22+)\n‚Ä¢ Use OpenVINO for Intel NPUs\n‚Ä¢ Enable DirectML for Windows NPU support\n‚Ä¢ Test with quantized models (INT8/FP16)".to_string();
        }
        
        // Greeting responses
        if content_lower.contains("hello") || content_lower.contains("hi ") || content_lower.starts_with("hi") {
            return "Hello! üëã I'm RIA AI Assistant. I'm currently running in intelligent demo mode and ready to help!\n\nüîß **Available capabilities:**\n‚Ä¢ Answer questions and provide explanations\n‚Ä¢ Help with coding and programming\n‚Ä¢ Discuss various topics\n‚Ä¢ Provide contextual assistance\n\nüí° Load an ONNX model from the üß† Models tab for enhanced AI inference!\n\nWhat would you like to know?".to_string();
        }
        
        // Status and model questions
        if content_lower.contains("model") || content_lower.contains("onnx") || content_lower.contains("status") {
            return "üß† **System Status Report**\n\n**Current State**: Demo Mode (ONNX Runtime issue)\n\n**‚ö†Ô∏è Issue Detected:**\n‚Ä¢ ONNX Runtime version mismatch\n‚Ä¢ System has v1.17.1, app requires v1.22+\n‚Ä¢ NPU cores detected but not accessible\n\n**üîß Solutions:**\n1. **Update ONNX Runtime:**\n   ```bash\n   pip install onnxruntime --upgrade\n   # or for GPU/NPU support:\n   pip install onnxruntime-openvino\n   ```\n\n2. **Alternative: Use system package manager**\n   ```bash\n   winget install Microsoft.ONNXRuntime\n   ```\n\n**‚úÖ What works now:**\n‚Ä¢ Intelligent demo responses\n‚Ä¢ Code examples and explanations\n‚Ä¢ Programming assistance\n‚Ä¢ NPU programming tutorials\n\n**üöÄ After fixing ONNX Runtime:**\n‚Ä¢ Real AI model inference\n‚Ä¢ NPU acceleration support\n‚Ä¢ Advanced reasoning capabilities\n\n*Demo mode is fully functional for learning and development!*".to_string();
        }
        
        // Specific requests for simple Rust apps
        if (content_lower.contains("write") || content_lower.contains("create") || content_lower.contains("simple")) && 
           (content_lower.contains("rust") && content_lower.contains("app")) {
            return "ü¶Ä **Simple Rust App Example**\n\nHere's a complete simple Rust application:\n\n```rust\nuse std::io;\nuse std::collections::HashMap;\n\n#[derive(Debug, Clone)]\nstruct Task {\n    id: u32,\n    description: String,\n    completed: bool,\n}\n\nstruct TaskManager {\n    tasks: HashMap<u32, Task>,\n    next_id: u32,\n}\n\nimpl TaskManager {\n    fn new() -> Self {\n        Self {\n            tasks: HashMap::new(),\n            next_id: 1,\n        }\n    }\n    \n    fn add_task(&mut self, description: String) -> u32 {\n        let id = self.next_id;\n        let task = Task {\n            id,\n            description,\n            completed: false,\n        };\n        self.tasks.insert(id, task);\n        self.next_id += 1;\n        id\n    }\n    \n    fn complete_task(&mut self, id: u32) -> bool {\n        if let Some(task) = self.tasks.get_mut(&id) {\n            task.completed = true;\n            true\n        } else {\n            false\n        }\n    }\n    \n    fn list_tasks(&self) {\n        if self.tasks.is_empty() {\n            println!(\"üìù No tasks yet!\");\n            return;\n        }\n        \n        println!(\"\\nüìã Your Tasks:\");\n        for task in self.tasks.values() {\n            let status = if task.completed { \"‚úÖ\" } else { \"‚è≥\" };\n            println!(\"{} {}. {}\", status, task.id, task.description);\n        }\n    }\n}\n\nfn main() {\n    println!(\"üöÄ Simple Rust Task Manager\");\n    println!(\"Commands: add <task>, complete <id>, list, quit\");\n    \n    let mut manager = TaskManager::new();\n    \n    loop {\n        print!(\"\\n> \");\n        let mut input = String::new();\n        io::stdin().read_line(&mut input).expect(\"Failed to read input\");\n        \n        let input = input.trim().to_lowercase();\n        let parts: Vec<&str> = input.split_whitespace().collect();\n        \n        match parts.get(0) {\n            Some(&\"add\") => {\n                if parts.len() > 1 {\n                    let task_desc = parts[1..].join(\" \");\n                    let id = manager.add_task(task_desc);\n                    println!(\"‚úÖ Added task #{}\", id);\n                } else {\n                    println!(\"‚ùå Usage: add <task description>\");\n                }\n            },\n            Some(&\"complete\") => {\n                if let Some(id_str) = parts.get(1) {\n                    if let Ok(id) = id_str.parse::<u32>() {\n                        if manager.complete_task(id) {\n                            println!(\"üéâ Task #{} completed!\", id);\n                        } else {\n                            println!(\"‚ùå Task #{} not found\", id);\n                        }\n                    } else {\n                        println!(\"‚ùå Invalid task ID\");\n                    }\n                } else {\n                    println!(\"‚ùå Usage: complete <task_id>\");\n                }\n            },\n            Some(&\"list\") => manager.list_tasks(),\n            Some(&\"quit\") => {\n                println!(\"üëã Goodbye!\");\n                break;\n            },\n            _ => println!(\"‚ùì Unknown command. Try: add, complete, list, quit\"),\n        }\n    }\n}\n```\n\n**To run this:**\n```bash\ncargo new simple_task_app\ncd simple_task_app\n# Replace src/main.rs with the code above\ncargo run\n```\n\n**This app demonstrates:**\n‚úÖ Structs and implementations\n‚úÖ HashMap for data storage\n‚úÖ User input handling\n‚úÖ Pattern matching\n‚úÖ Error handling\n‚úÖ Command-line interface\n\n**Try these commands:**\n‚Ä¢ `add Buy groceries`\n‚Ä¢ `add Write code`\n‚Ä¢ `list`\n‚Ä¢ `complete 1`\n‚Ä¢ `quit`".to_string();
        }
        
        // Programming and coding (generic fallback)
        if content_lower.contains("code") || content_lower.contains("program") || content_lower.contains("function") || content_lower.contains("rust") || content_lower.contains("python") || content_lower.contains("javascript") {
            return format!("üîß **Programming Help Available**\n\nYou asked: *\"{}\"*\n\n**I can provide:**\n‚Ä¢ Complete code examples (like task managers, calculators, etc.)\n‚Ä¢ Algorithm implementations\n‚Ä¢ Debugging assistance\n‚Ä¢ Best practices and patterns\n‚Ä¢ Code reviews and explanations\n\n**Specific help available for:**\n‚Ä¢ **Simple Rust apps** - Complete working examples\n‚Ä¢ **NPU integration** - Hardware acceleration code\n‚Ä¢ **System programming** - File I/O, networking, etc.\n‚Ä¢ **Data structures** - HashMaps, Vectors, custom types\n\n**Be more specific for better help:**\n‚Ä¢ \"Write a simple Rust app\" ‚Üí Complete working code\n‚Ä¢ \"NPU code example\" ‚Üí Hardware acceleration tutorial\n‚Ä¢ \"File handling in Rust\" ‚Üí I/O examples\n‚Ä¢ \"Error handling patterns\" ‚Üí Result/Option examples", 
                if original_content.len() > 100 { format!("{}...", &original_content[..97]) } else { original_content.to_string() });
        }
        
        // Questions and explanations
        if content_lower.starts_with("what") || content_lower.starts_with("how") || content_lower.starts_with("why") || content_lower.contains("explain") {
            return format!("ü§î **Great question!** You asked: *\"{}\"*\n\n**Demo Response:**\nI'd be happy to help explain this topic! In demo mode, I can provide contextual guidance and point you in the right direction.\n\n**For detailed explanations:**\n‚Ä¢ Load an ONNX model for comprehensive responses\n‚Ä¢ I can still provide helpful context and suggestions\n‚Ä¢ Ask follow-up questions for more specific guidance\n\n*What aspect would you like me to focus on?*", 
                if original_content.len() > 100 { format!("{}...", &original_content[..97]) } else { original_content.to_string() });
        }
        
        // Help and assistance requests
        if content_lower.contains("help") || content_lower.contains("assist") || content_lower.contains("support") {
            return "üÜò **Help & Support**\n\n**I'm here to help!** Current capabilities in demo mode:\n\n**‚úÖ Available:**\n‚Ä¢ Answer questions and provide context\n‚Ä¢ Coding assistance and examples\n‚Ä¢ General explanations and guidance\n‚Ä¢ Topic discussions and brainstorming\n\n**‚ö° Enhanced with ONNX models:**\n‚Ä¢ Advanced AI reasoning\n‚Ä¢ Detailed technical responses\n‚Ä¢ Complex problem solving\n‚Ä¢ Specialized domain knowledge\n\n**üîß Need technical support?**\n‚Ä¢ Check the Settings ‚öôÔ∏è for configuration\n‚Ä¢ Visit Models üß† to load AI capabilities\n‚Ä¢ Use Ctrl+H for keyboard shortcuts\n\n*How can I specifically help you today?*".to_string();
        }
        
        // Math and calculations
        if content_lower.contains("calculate") || content_lower.contains("math") || content_lower.contains("equation") {
            return format!("üßÆ **Math & Calculations**\n\nYou mentioned: *\"{}\"*\n\n**Demo Mode Capabilities:**\n‚Ä¢ Basic math explanations\n‚Ä¢ Formula breakdowns\n‚Ä¢ Calculation guidance\n‚Ä¢ Mathematical concepts\n\n**üìä Enhanced with models:**\n‚Ä¢ Complex calculations\n‚Ä¢ Advanced mathematics\n‚Ä¢ Statistical analysis\n‚Ä¢ Mathematical proofs\n\n*What mathematical concept can I help explain?*", 
                if original_content.len() > 80 { format!("{}...", &original_content[..77]) } else { original_content.to_string() });
        }
        
        // Default intelligent response based on context
        let message_count = messages.len();
        let is_follow_up = message_count > 2;
        
        if is_follow_up {
            format!("üí¨ **Continuing our conversation...**\n\nYou said: *\"{}\"*\n\n**Context-aware response:**\nI'm following our discussion and ready to dive deeper! In demo mode, I can provide thoughtful responses based on our conversation flow.\n\n**üîÑ For enhanced continuity:**\n‚Ä¢ Load an ONNX model for advanced context understanding\n‚Ä¢ I maintain conversation awareness in demo mode\n‚Ä¢ Feel free to ask follow-up questions!\n\n*What would you like to explore further?*", 
                if original_content.len() > 100 { format!("{}...", &original_content[..97]) } else { original_content.to_string() })
        } else {
            format!("ü§ñ **Demo Mode Response**\n\nYou said: *\"{}\"*\n\n**I'm actively listening!** While in demo mode, I can:\n‚Ä¢ Provide contextual responses\n‚Ä¢ Offer relevant suggestions\n‚Ä¢ Help brainstorm ideas\n‚Ä¢ Give guidance on various topics\n\n**üí° Tips:**\n‚Ä¢ Ask specific questions for better responses\n‚Ä¢ Try topics like coding, explanations, or help\n‚Ä¢ Load a model from üß† Models for advanced AI\n\n*Feel free to ask me anything - I'm here to help!*", 
                if original_content.len() > 100 { format!("{}...", &original_content[..97]) } else { original_content.to_string() })
        }
    }
}

impl InferenceEngine {
    pub fn new() -> Self {
        Self {
            providers: Vec::new(),
            active_provider: None,
            config: Arc::new(RwLock::new(InferenceConfig::default())),
        }
    }

    pub async fn add_provider(&mut self, provider: Box<dyn AIProvider + Send + Sync>) {
        self.providers.push(provider);
    }

    /// Synchronous helper to add a provider and return its index
    pub fn add_provider_sync(&mut self, provider: Box<dyn AIProvider + Send + Sync>) -> usize {
        self.providers.push(provider);
        self.providers.len() - 1
    }

    pub async fn set_active_provider(&mut self, index: usize) -> Result<()> {
        if index >= self.providers.len() {
            return Err(anyhow::anyhow!("Provider index out of bounds"));
        }
        self.active_provider = Some(index);
        Ok(())
    }

    /// Synchronous helper to set the active provider
    pub fn set_active_provider_sync(&mut self, index: usize) -> Result<()> {
        if index >= self.providers.len() {
            return Err(anyhow::anyhow!("Provider index out of bounds"));
        }
        self.active_provider = Some(index);
        Ok(())
    }

    /// Check if an active provider is set
    pub fn has_active_provider(&self) -> bool {
        self.active_provider.is_some()
    }

    pub async fn get_available_providers(&self) -> Vec<String> {
        self.providers
            .iter()
            .enumerate()
            .filter_map(|(i, provider)| {
                if provider.is_available() {
                    Some(format!("{}: {}", i, provider.name()))
                } else {
                    None
                }
            })
            .collect()
    }

    pub async fn generate_response(&mut self, messages: &[ChatMessage]) -> Result<ChatMessage> {
        let provider_idx = self.active_provider
            .ok_or_else(|| anyhow::anyhow!("No active provider set"))?;

        let start_time = std::time::Instant::now();
        
        let response_content = {
            let provider = &mut self.providers[provider_idx];
            provider.generate_response(messages)?
        };
        
        let inference_time = start_time.elapsed().as_secs_f64();

        Ok(ChatMessage {
            id: uuid::Uuid::new_v4().to_string(),
            content: response_content,
            role: MessageRole::Assistant,
            timestamp: chrono::Utc::now(),
            model_used: Some(self.providers[provider_idx].name().to_string()),
            inference_time: Some(inference_time),
        })
    }

    pub async fn update_config(&self, config: InferenceConfig) {
        let mut current_config = self.config.write().await;
        *current_config = config;
    }

    pub async fn get_config(&self) -> InferenceConfig {
        self.config.read().await.clone()
    }

    /// Generate a response and stream it back in chunks over a channel.
    /// This scaffolds streaming by chunking a full response; later we can replace
    /// this with true token-by-token streaming from the provider.
    pub fn generate_response_stream(
        &mut self,
        messages: &[ChatMessage],
        chunk_chars: usize,
        delay_ms: u64,
    ) -> Result<mpsc::Receiver<String>> {
        let provider_idx = self
            .active_provider
            .ok_or_else(|| anyhow::anyhow!("No active provider set"))?;

        // Generate the full response synchronously to avoid threading the provider
        let response_content = {
            let provider = &mut self.providers[provider_idx];
            provider.generate_response(messages)?
        };

        let (tx, rx) = mpsc::channel(32);

        // Stream the response in small chunks to simulate token streaming
        tokio::spawn(async move {
            let mut buf = String::new();
            let mut count = 0usize;

            for ch in response_content.chars() {
                buf.push(ch);
                count += 1;

                if count >= chunk_chars {
                    if tx.send(buf.clone()).await.is_err() {
                        return; // receiver dropped
                    }
                    buf.clear();
                    count = 0;
                    if delay_ms > 0 {
                        sleep(Duration::from_millis(delay_ms)).await;
                    }
                }
            }

            if !buf.is_empty() {
                let _ = tx.send(buf).await;
            }
        });

        Ok(rx)
    }

    /// Placeholder: generate streaming using logits sampling (future real logits extraction)
    pub fn generate_response_stream_sampled(&mut self, messages: &[ChatMessage], max_tokens: usize, delay_ms: u64) -> Result<mpsc::Receiver<String>> {
        use crate::ai::sampler::{LogitsSampler, SamplerConfig, SamplingStrategy};
        let provider_idx = self.active_provider.ok_or_else(|| anyhow::anyhow!("No active provider set"))?;
        let mut sampler = LogitsSampler::new(SamplerConfig { temperature: 0.8, strategy: SamplingStrategy::TopP { p: 0.95 } });
        // For now fabricate logits vocabulary of limited size
        let vocab = ["the","rust","ai","model","is","ready","and","responding","to","your","message","now","!","assistant"];
        let (tx, rx) = mpsc::channel(32);
        let mut generated = String::new();
        let base_prompt = messages.iter().filter(|m| matches!(m.role, MessageRole::User)).map(|m| &m.content).last().cloned().unwrap_or_default();
        tokio::spawn(async move {
            for step in 0..max_tokens {                
                // Fake logits: random values
                let mut logits: Vec<f32> = (0..vocab.len()).map(|_| rand::random::<f32>()).collect();
                // Bias to encourage ending
                if step > max_tokens/2 { logits[vocab.len()-1] += 1.0; }
                if let Some(idx) = sampler.sample(&logits) {                    
                    let token = vocab[idx];
                    if token == "assistant" && step < 2 { continue; }
                    generated.push_str(token);
                    generated.push(' ');
                    if tx.send(generated.clone()).await.is_err() { break; }
                    if token == "!" { break; }
                } else { break; }
                tokio::time::sleep(Duration::from_millis(delay_ms)).await;
            }
        });
        Ok(rx)
    }
}
